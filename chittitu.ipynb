{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "c9580be6-0d80-4bfd-8cc5-347502eaff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chittitu ipynb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "b6e235c5-35bd-4bdb-8eb0-63dbeea4db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import base64\n",
    "\n",
    "if SparkContext._active_spark_context:\n",
    "    SparkContext._active_spark_context.stop()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WeatherDataAnalysis\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "f73f75a6-59eb-4d3b-9949-f1b22cddba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 - Load the CSV files and display the count of each dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "e0b84ea6-c1f9-49be-9c7b-7de42acc5109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files: 19\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"*/*.csv\")\n",
    "file_count = len(df.inputFiles())\n",
    "\n",
    "print(f\"Total number of files: {file_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "59f34a26-86dc-4d23-be18-994811a79f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cincinnati dataset count: 3585\n",
      "Florida dataset count: 2483\n"
     ]
    }
   ],
   "source": [
    "years = range(2015, 2025)\n",
    "cincinnati_path = [f\"{year}/72429793812.csv\" for year in years]\n",
    "florida_path = [f\"{year}/99495199999.csv\" for year in years if year != 2016]\n",
    "\n",
    "data_cin = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(cincinnati_path)\n",
    "data_flo = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(florida_path)\n",
    "\n",
    "print(\"Cincinnati dataset count:\", data_cin.count())\n",
    "print(\"Florida dataset count:\", data_flo.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "84e1623a-6fe1-4353-a4b3-6565618ea42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for each dataset from Cincinnati:\n",
      "+----+-----+\n",
      "|Year|count|\n",
      "+----+-----+\n",
      "|2015|  365|\n",
      "|2016|  366|\n",
      "|2017|  365|\n",
      "|2018|  365|\n",
      "|2019|  365|\n",
      "|2020|  366|\n",
      "|2021|  365|\n",
      "|2022|  365|\n",
      "|2023|  365|\n",
      "|2024|  298|\n",
      "+----+-----+\n",
      "\n",
      "Count for each dataset from Florida:\n",
      "+----+-----+\n",
      "|Year|count|\n",
      "+----+-----+\n",
      "|2015|  355|\n",
      "|2017|  283|\n",
      "|2018|  363|\n",
      "|2019|  345|\n",
      "|2020|  365|\n",
      "|2021|  104|\n",
      "|2022|  259|\n",
      "|2023|  276|\n",
      "|2024|  133|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import input_file_name, regexp_extract\n",
    "\n",
    "cin = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"*/72429793812.csv\") \n",
    "cin = cin.withColumn(\"Year\", input_file_name())\n",
    "cin = cin.withColumn(\"Year\", regexp_extract(\"Year\", r\"(\\d{4})\", 1))\n",
    "\n",
    "flo = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"*/99495199999.csv\") \n",
    "flo = flo.withColumn(\"Year\", input_file_name())\n",
    "flo = flo.withColumn(\"Year\", regexp_extract(\"Year\", r\"(\\d{4})\", 1))\n",
    "\n",
    "print(\"Count for each dataset from Cincinnati:\")\n",
    "cin.groupBy(\"Year\").count().orderBy(\"Year\").show()\n",
    "print(\"Count for each dataset from Florida:\")\n",
    "flo.groupBy(\"Year\").count().orderBy(\"Year\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "2317737f-f755-4c4d-8969-041358d1d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 - Find the hottest day (column MAX) for each year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "4c9d35e1-b8c8-4caa-b255-bd933bff3f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 2015\n",
      "Station Code: 72429793812\n",
      "Station Name: CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US\n",
      "Hottest Date: 12-06-2015\n",
      "Highest Temperature: 91.9\n",
      "------------------------------\n",
      "Year: 2016\n",
      "Station Code: 72429793812\n",
      "Station Name: CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US\n",
      "Hottest Date: 24-07-2016\n",
      "Highest Temperature: 93.9\n",
      "------------------------------\n",
      "Year: 2017\n",
      "Station Code: 72429793812\n",
      "Station Name: CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US\n",
      "Hottest Date: 22-07-2017\n",
      "Highest Temperature: 91.9\n",
      "------------------------------\n",
      "Year: 2018\n",
      "Station Code: 72429793812\n",
      "Station Name: CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US\n",
      "Hottest Date: 04-07-2018\n",
      "Highest Temperature: 96.1\n",
      "------------------------------\n",
      "Year: 2019\n",
      "Station Code: 72429793812\n",
      "Station Name: CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US\n",
      "Hottest Date: 30-09-2019\n",
      "Highest Temperature: 95.0\n",
      "------------------------------\n",
      "Year: 2020\n",
      "Station Code: 72429793812\n",
      "Station Name: CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US\n",
      "Hottest Date: 05-07-2020\n",
      "Highest Temperature: 93.9\n",
      "------------------------------\n",
      "Year: 2021\n",
      "Station Code: 72429793812\n",
      "Station Name: CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US\n",
      "Hottest Date: 12-08-2021\n",
      "Highest Temperature: 95.0\n",
      "------------------------------\n",
      "Year: 2022\n",
      "Station Code: 72429793812\n",
      "Station Name: CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US\n",
      "Hottest Date: 14-06-2022\n",
      "Highest Temperature: 96.1\n",
      "------------------------------\n",
      "Year: 2023\n",
      "Station Code: 72429793812\n",
      "Station Name: CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US\n",
      "Hottest Date: 23-08-2023\n",
      "Highest Temperature: 96.1\n",
      "------------------------------\n",
      "Year: 2024\n",
      "Station Code: 72429793812\n",
      "Station Name: CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US\n",
      "Hottest Date: 30-08-2024\n",
      "Highest Temperature: 100.9\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, col, date_format\n",
    "\n",
    "for year_val in range(2015, 2025):\n",
    "    directory_path = f\"{year_val}\"\n",
    "    csv_file1 = \"/72429793812.csv\"  \n",
    "    csv_file2 = \"/99495199999.csv\" \n",
    "\n",
    "    path_to_file1 = f\"{directory_path}{csv_file1}\"\n",
    "    path_to_file2 = f\"{directory_path}{csv_file2}\"\n",
    "\n",
    "    try:\n",
    "        data_cin2 = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(path_to_file1)\n",
    "        data_cin2 = data_cin2.withColumn(\"Year\", year(col(\"DATE\")))\n",
    "        max_cin_row = data_cin2.filter((data_cin2[\"Year\"] == year_val) & (data_cin2[\"MAX\"] != 9999.9)) \\\n",
    "                                   .orderBy(col(\"MAX\").desc()) \\\n",
    "                                   .first()\n",
    "        \n",
    "        if year_val==2016:\n",
    "             print(f\"Year: {year_val}\")\n",
    "             print(\"Station Code: 72429793812\")\n",
    "             print(\"Station Name: CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US\")\n",
    "             reversed_cin_date = data_cin2.select(date_format(col(\"DATE\"), \"dd-MM-yyyy\")).filter(col(\"DATE\") == max_cin_row['DATE']).first()[0]\n",
    "             print(f\"Hottest Date: {reversed_cin_date}\")\n",
    "             print(f\"Highest Temperature: {max_cin_row['MAX']}\")\n",
    "             print(\"-\" * 30)\n",
    "        else:\n",
    "            data_flo = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(path_to_file2)\n",
    "            data_flo = data_flo.withColumn(\"Year\", year(col(\"DATE\")))\n",
    "            max_flo_row = data_flo.filter((data_flo[\"Year\"] == year_val) & (data_flo[\"MAX\"] != 9999.9)) \\\n",
    "                                       .orderBy(col(\"MAX\").desc()) \\\n",
    "                                       .first()\n",
    "            \n",
    "            if max_cin_row < max_flo_row:\n",
    "                print(f\"Year: {year_val}\")\n",
    "                print(\"Station Code: 72429793812\")\n",
    "                print(\"Station Name: CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US\")\n",
    "                reversed_cin_date = data_cin2.select(date_format(col(\"DATE\"), \"dd-MM-yyyy\")).filter(col(\"DATE\") == max_cin_row['DATE']).first()[0]\n",
    "                print(f\"Hottest Date: {reversed_cin_date}\")\n",
    "                print(f\"Highest Temperature: {max_cin_row['MAX']}\")\n",
    "                if year_val < 2024:\n",
    "                    print(\"-\" * 30)\n",
    "            else:\n",
    "                print(f\"Year: {year_val}\")\n",
    "                print(\"Station Code: 99495199999\")\n",
    "                print(\"Station Name: SEBASTIAN INLET STATE PARK, FL US\")\n",
    "                reversed_cin_date = data_cin2.select(date_format(col(\"DATE\"), \"dd-MM-yyyy\")).filter(col(\"DATE\") == max_cin_row['DATE']).first()[0]\n",
    "                print(f\"Hottest Date: {reversed_cin_date}\")\n",
    "                print(f\"Highest Temperature: {max_flo_row['MAX']}\")\n",
    "                if year_val < 2024:\n",
    "                    print(\"-\" * 30)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing files for year {year_val}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "3dfcb62c-8120-450e-b9e3-182d051b19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 - Find the coldest day (column MIN) for the month of March across all years (2015-2024) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "1b4bf335-7065-477c-a1e3-27cd118940cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coldest day in March across all years:\n",
      "Station Code: 72429793812\n",
      "Station Name: CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US\n",
      "Coldest Date: 06-03-2015\n",
      "Lowest Temperature: 3.2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, year, month, mean, stddev, count, when, date_format, lit\n",
    "coldest_march_cin = data_cin.filter((month(col(\"DATE\")) == 3) & (col(\"MIN\") != 9999.9)) \\\n",
    "    .orderBy(col(\"MIN\").asc()) \\\n",
    "    .select(\"STATION\", \"NAME\", date_format(\"DATE\", \"dd-MM-yyyy\").alias(\"Formatted_Date\"), \"MIN\").first()\n",
    "\n",
    "coldest_march_flo = data_flo.filter((month(col(\"DATE\")) == 3) & (col(\"MIN\") != 9999.9)) \\\n",
    "    .orderBy(col(\"MIN\").asc()) \\\n",
    "    .select(\"STATION\", \"NAME\", date_format(\"DATE\", \"dd-MM-yyyy\").alias(\"Formatted_Date\"), \"MIN\").first()\n",
    "if coldest_march_cin < coldest_march_flo:\n",
    "    print(\"Coldest day in March across all years:\")\n",
    "    print(f\"Station Code: {coldest_march_cin['STATION']}\")\n",
    "    print(f\"Station Name: {coldest_march_cin['NAME']}\")\n",
    "    print(f\"Coldest Date: {coldest_march_cin['Formatted_Date']}\")\n",
    "    print(f\"Lowest Temperature: {coldest_march_cin['MIN']}\")\n",
    "else:\n",
    "    print(\"Coldest day in March across all years:\")\n",
    "    print(f\"Station Code: {coldest_march_flo['STATION']}\")\n",
    "    print(f\"Station Name: {coldest_march_flo['NAME']}\")\n",
    "    print(f\"Coldest Date: {coldest_march_flo['Formatted_Date']}\")\n",
    "    print(f\"Lowest Temperature: {coldest_march_flo['MIN']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "1b681969-7815-407a-bd29-d08b136b9add",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 - Find the year with the most precipitation for Cincinnati and Florida "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "5595c4cb-9dd8-4a4c-8d97-6371970bcf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most precipitation for Cincinnati was 5.49 in the year 2024\n",
      "The most precipitation for Florida was 0.00 in the year 2024\n"
     ]
    }
   ],
   "source": [
    "precipitation_data_cin = data_cin.groupBy(year(\"DATE\").alias(\"Year\")).agg(mean(\"PRCP\").alias(\"Average_PRCP\"))\n",
    "max_precipitation_year_cin = precipitation_data_cin.orderBy(col(\"Average_PRCP\").desc()).first()\n",
    "\n",
    "precipitation_data_flo = data_flo.groupBy(year(\"DATE\").alias(\"Year\")).agg(mean(\"PRCP\").alias(\"Average_PRCP\"))\n",
    "max_precipitation_year_flo = precipitation_data_flo.orderBy(col(\"Average_PRCP\").desc()).first()\n",
    "\n",
    "print(f\"The most precipitation for Cincinnati was {max_precipitation_year_cin['Average_PRCP']:.2f} in the year {max_precipitation_year_cin['Year']}\")\n",
    "if ({max_precipitation_year_flo['Average_PRCP']}==0):\n",
    "    print(\"The precipitation for Florida was the same for years 2015-2024 with a precipitation of 0\")\n",
    "else:\n",
    "    print(f\"The most precipitation for Florida was {max_precipitation_year_flo['Average_PRCP']:.2f} in the year {max_precipitation_year_flo['Year']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "adde0716-fb8e-4ff2-a67b-5f9f059f2506",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 - Count the percentage of missing values for wind gust (column GUST) for Cincinnati and Florida in the year 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "a223f721-7ef0-414b-a0a1-007ba4d41853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cincinnati 2024 - Missing GUST %: 39.60\n",
      "Florida 2024 - Missing GUST %: 100.00\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "cinci_2024 = spark.read.option(\"header\", \"true\").csv(\"2024/72429793812.csv\") \n",
    "florida_2024 = spark.read.option(\"header\", \"true\").csv(\"2024/99495199999.csv\")\n",
    "\n",
    "cinci_total = cinci_2024.count()\n",
    "florida_total = florida_2024.count()\n",
    "\n",
    "cinci_missing_gust = cinci_2024.filter(col(\"GUST\") == 999.9).count()\n",
    "florida_missing_gust = florida_2024.filter(col(\"GUST\") == 999.9).count()\n",
    "\n",
    "cinci_missing_percent = (cinci_missing_gust / cinci_total) * 100\n",
    "florida_missing_percent = (florida_missing_gust / florida_total) * 100\n",
    "\n",
    "print(f\"Cincinnati 2024 - Missing GUST %: {cinci_missing_percent:0.2f}\")\n",
    "print(f\"Florida 2024 - Missing GUST %: {florida_missing_percent:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "b21017b2-6a1c-406f-ae0e-0e4cd3c6155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 - Find the mean, median, mode, and standard deviation of the temperature (column TEMP) for Cincinnati in each month for the year 2020 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "764aa161-5f63-498d-9785-3d29ed218855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+-----------+---------+\n",
      "|Month|         Mean_TEMP|       StdDev_TEMP|Median_TEMP|Mode_TEMP|\n",
      "+-----+------------------+------------------+-----------+---------+\n",
      "|    1|37.945161081129505| 8.345810838316384|       37.7|     24.7|\n",
      "|    2| 36.58965525133856| 7.901597947537755|       36.0|     30.8|\n",
      "|    3|  49.0741934007214|  8.77940669347644|       47.8|     47.8|\n",
      "|    4| 51.77999992370606|7.3131621276074465|       51.0|     46.8|\n",
      "|    5| 60.89032290058751| 9.314768319579512|       63.7|     73.9|\n",
      "|    6| 72.54666570027669|4.8999458590264515|       73.7|     74.2|\n",
      "|    7|  77.6000001968876| 2.337947626620972|       77.9|     78.4|\n",
      "|    8| 73.34516143798828|3.4878690606063563|       73.7|     67.4|\n",
      "|    9| 66.09999961853028| 7.118261579669542|       65.8|     72.7|\n",
      "|   10| 55.19354851015152|6.7286914818367975|       54.0|     61.6|\n",
      "|   11| 48.00333340962728| 6.825938707865554|       47.7|     47.7|\n",
      "|   12| 35.99354830095845|6.6427872766495755|       35.2|     37.4|\n",
      "+-----+------------------+------------------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, month, avg, stddev, expr, approx_count_distinct\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"2020/72429793812.csv\")\n",
    "df = df.withColumn(\"TEMP\", col(\"TEMP\").cast(\"float\")).withColumn(\"DATE\", col(\"DATE\").cast(\"date\"))\n",
    "\n",
    "results = df.withColumn(\"Month\", month(\"DATE\")) \\\n",
    "    .groupBy(\"Month\") \\\n",
    "    .agg(\n",
    "        avg(\"TEMP\").alias(\"Mean_TEMP\"),\n",
    "        stddev(\"TEMP\").alias(\"StdDev_TEMP\"),\n",
    "        expr(\"percentile_approx(TEMP, 0.5)\").alias(\"Median_TEMP\"),  \n",
    "        expr(\"mode(TEMP)\").alias(\"Mode_TEMP\") \n",
    "    ) \\\n",
    "    .orderBy(\"Month\")\n",
    "\n",
    "results.show(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "d566ad40-0c6b-4fc6-9813-a9cf7490d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8 - Find the top 10 days with the lowest Wind Chill for Cincinnati in 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "972a4e4b-440f-4377-8387-7d4ea2fd68d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+----+----+-------------------+\n",
      "|      DATE|    STATION|                NAME|TEMP|WDSP|         Wind_Chill|\n",
      "+----------+-----------+--------------------+----+----+-------------------+\n",
      "|2017-01-07|72429793812|CINCINNATI MUNICI...|10.5| 7.0|-0.4140156367932173|\n",
      "|2017-12-31|72429793812|CINCINNATI MUNICI...|11.0| 5.3| 2.0339764741541018|\n",
      "|2017-12-27|72429793812|CINCINNATI MUNICI...|13.0| 5.8| 3.8206452986638073|\n",
      "|2017-12-28|72429793812|CINCINNATI MUNICI...|13.6| 5.8|  4.533355513517824|\n",
      "|2017-01-06|72429793812|CINCINNATI MUNICI...|13.6| 5.5|  4.868933492954463|\n",
      "|2017-01-08|72429793812|CINCINNATI MUNICI...|15.9| 5.2|  7.929747979856229|\n",
      "|2017-12-25|72429793812|CINCINNATI MUNICI...|25.8|13.5| 14.285112249501509|\n",
      "|2017-12-30|72429793812|CINCINNATI MUNICI...|21.6| 5.3| 14.539211503699956|\n",
      "|2017-01-05|72429793812|CINCINNATI MUNICI...|22.2| 5.8| 14.748862551376547|\n",
      "|2017-12-26|72429793812|CINCINNATI MUNICI...|23.3| 6.2| 15.688977064714743|\n",
      "+----------+-----------+--------------------+----+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"2017/72429793812.csv\")\n",
    "\n",
    "df = df.withColumn(\"TEMP\", col(\"TEMP\").cast(\"float\")).withColumn(\"WDSP\", col(\"WDSP\").cast(\"float\")).withColumn(\"DATE\", col(\"DATE\").cast(\"date\"))\n",
    "\n",
    "df_filtered = df.filter((col(\"STATION\") == \"72429793812\") & (col(\"DATE\").between(\"2017-01-01\", \"2017-12-31\")) & (col(\"TEMP\") < 50) & (col(\"WDSP\") > 3))\n",
    "\n",
    "df_wind_chill = df_filtered.withColumn(\n",
    "    \"Wind_Chill\",\n",
    "    35.74 + (0.6215 * col(\"TEMP\")) - (35.75 * (col(\"WDSP\")**0.16)) + (0.4275 * col(\"TEMP\") * (col(\"WDSP\")**0.16))\n",
    ")\n",
    "\n",
    "top_10_lowest_wind_chill = df_wind_chill.select(\"DATE\", \"STATION\", \"NAME\", \"TEMP\", \"WDSP\", \"Wind_Chill\") \\\n",
    "    .orderBy(\"Wind_Chill\") \\\n",
    "    .limit(10)\n",
    "\n",
    "top_10_lowest_wind_chill.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "95791a78-4a27-4421-9d80-8d305354f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9 - Investigate how many days had extreme weather conditions for Florida (fog, rain, snow, etc.) using the FRSHTT column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "474eb569-6f61-41d4-8c10-31f0f7478c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of days with extreme weather conditions in Florida: 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# for rows with any extreme condition occurred in FRSHTT\n",
    "extreme_weather_days = data_flo.filter(\n",
    "    (col(\"FRSHTT\").substr(1, 1) == '1') |  #for fog\n",
    "    (col(\"FRSHTT\").substr(2, 1) == '1') |  #for rain or drizzle\n",
    "    (col(\"FRSHTT\").substr(3, 1) == '1') |  #for snow\n",
    "    (col(\"FRSHTT\").substr(4, 1) == '1') |  #for hail\n",
    "    (col(\"FRSHTT\").substr(5, 1) == '1') |  #for thunder\n",
    "    (col(\"FRSHTT\").substr(6, 1) == '1')    #for tornado or funnel cloud\n",
    ")\n",
    "\n",
    "extreme_weather_count = extreme_weather_days.count()\n",
    "print(f\"Number of days with extreme weather conditions in Florida: {extreme_weather_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "9fb5186b-c018-4f29-9958-8f7d0c133125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days with extreme weather in Florida: 0\n"
     ]
    }
   ],
   "source": [
    "extreme_weather_florida = data_flo.filter((col(\"FRSHTT\") != \"000000\")).count()\n",
    "print(\"Days with extreme weather in Florida:\", extreme_weather_florida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "fbcc3963-1f41-4ae1-a45d-94cff2cbb0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 - Predict the maximum Temperature for Cincinnati for November and December 2024, based on the previous 2 years of weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "c0d9971b-319d-440c-8046-4c34157bd146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+\n",
      "|     features|November Prediction|\n",
      "+-------------+-------------------+\n",
      "|[2024.0,11.0]|    66.908455259736|\n",
      "+-------------+-------------------+\n",
      "\n",
      "+-------------+-------------------+\n",
      "|     features|December Prediction|\n",
      "+-------------+-------------------+\n",
      "|[2024.0,12.0]| 57.475203227402744|\n",
      "+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Load datasets for 2022, 2023, and 2024\n",
    "data_2022 = spark.read.csv(\"2022/72429793812.csv\", header=True, inferSchema=True)\n",
    "data_2023 = spark.read.csv(\"2023/72429793812.csv\", header=True, inferSchema=True)\n",
    "data_2024 = spark.read.csv(\"2024/72429793812.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Combine datasets into one DataFrame\n",
    "data_cin1 = data_2022.union(data_2023).union(data_2024)\n",
    "\n",
    "# Filter out rows with missing \"MAX\" values (where MAX == 9999.9)\n",
    "data_cin_filtered = data_cin1.filter(col(\"MAX\") != 9999.9)\n",
    "\n",
    "# Add Year and Month columns based on the DATE column\n",
    "data_cin_filtered = data_cin_filtered.withColumn(\"Year\", year(col(\"DATE\"))).withColumn(\"Month\", month(col(\"DATE\")))\n",
    "\n",
    "# Filter data for November and December of 2022 and 2023\n",
    "nov_dec_data = data_cin_filtered.filter(col(\"Month\").isin(11, 12)).select(\"MAX\", \"Year\", \"Month\")\n",
    "\n",
    "# Assemble features for linear regression (Year and Month)\n",
    "assembler = VectorAssembler(inputCols=[\"Year\", \"Month\"], outputCol=\"features\")\n",
    "assembled_data = assembler.transform(nov_dec_data.na.drop())\n",
    "\n",
    "# Split data for training (only use data before 2024)\n",
    "train_data = assembled_data.filter(col(\"Year\") < 2024)\n",
    "\n",
    "# Initialize and fit the Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"MAX\")\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# Create test data for November and December 2024 with different months\n",
    "nov_test_data = spark.createDataFrame([(2024, 11)], [\"Year\", \"Month\"])\n",
    "dec_test_data = spark.createDataFrame([(2024, 12)], [\"Year\", \"Month\"])\n",
    "\n",
    "# Assemble features for the November and December test data\n",
    "nov_assembled = assembler.transform(nov_test_data)\n",
    "dec_assembled = assembler.transform(dec_test_data)\n",
    "\n",
    "# Make predictions for November and December 2024\n",
    "nov_predictions = model.transform(nov_assembled)\n",
    "dec_predictions = model.transform(dec_assembled)\n",
    "\n",
    "# Display predictions for November and December 2024\n",
    "nov_predictions_with_watermark = nov_predictions.select(\"features\", \"prediction\") \\\n",
    "    .withColumnRenamed(\"prediction\", \"November Prediction\").show()\n",
    "\n",
    "# Add watermark to December predictions\n",
    "dec_predictions_with_watermark = dec_predictions.select(\"features\", \"prediction\") \\\n",
    "    .withColumnRenamed(\"prediction\", \"December Prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac279bb-5726-4be4-8559-dfe911b3f22e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
